# Spoken2Sign Pipeline (49-Landmark Sign Language Skeleton)

A clean, modular, SLRT-style Python pipeline for generating sign-language skeleton animations (49 MediaPipe Holistic landmarks) from spoken text.

**Now features a Scientific Verification Module (Ground Truth Comparison & Quantitative Metrics).**

---

## ğŸŒ Overview

This project implements a functional **Spoken-to-Sign (S2S)** demonstration system with a built-in validation loop:

**English text â†’ Gloss sequence â†’ Preprocessed keypoints â†’ Skeleton animation (MP4)**

### Key Features:

* **49 MediaPipe Holistic landmarks** (21 left hand, 21 right hand, 7 upper body).
* **High-Fidelity Rendering:** Professional "stacked-line" visuals with **Rainbow Finger Encoding** for distinct articulation clarity.
* **Scientific Verification:** A side-by-side comparison module that validates AI output against human Ground Truth videos.
* **Quantitative Metrics:** Built-in evaluation of Geometric Accuracy (MPJPE) and Visual Fidelity (FID).

---

## ğŸ“ Repository Structure

```text
spoken2sign-pipeline/
â”‚
â”œâ”€â”€ compare.py             # ğŸ”¬ Scientific Validation (AI vs. Human Side-by-Side)
â”œâ”€â”€ evaluate.py            # ğŸ“Š Quantitative Metrics (MPJPE, MPJAE, DTW, FID)
â”œâ”€â”€ render_gt.py           # ğŸ¬ Ground Truth Renderer (High-Fidelity)
â”‚
â”œâ”€â”€ modules/
â”‚   â”œâ”€â”€ loader.py          # Load PKL keypoints & gloss CSV
â”‚   â”œâ”€â”€ preprocess.py      # Velocity filtering, interpolation, smoothing
â”‚   â”œâ”€â”€ translate.py       # English â†’ gloss sequence
â”‚   â”œâ”€â”€ builder.py         # Gloss â†’ keypoint sequence builder
â”‚   â”œâ”€â”€ renderer.py        # High-quality rainbow skeleton renderer
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ run_pipeline.py    # Main pipeline: text â†’ MP4 animation
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ default.yaml       # File paths & rendering settings
â”‚
â”œâ”€â”€ datasets/
â”‚   â”œâ”€â”€ holistic_49_keypoints.pkl  # Extracted pose data
â”‚   â””â”€â”€ gloss_map.csv              # Mapping: Gloss ID <-> Video ID
â”‚
â”œâ”€â”€ output/
â”‚   â”œâ”€â”€ hello_world.mp4    # Generated AI Video
â”‚   â””â”€â”€ compare_hello.mp4  # Proof Video (Side-by-Side)
â”‚
â””â”€â”€ README.md

```

---

## ğŸ”§ Installation

Clone the repository:

```bash
git clone https://github.com/<your-username>/spoken2sign-pipeline
cd spoken2sign-pipeline

```

Install required packages:

```bash
pip install numpy matplotlib scipy pyyaml torch torchvision opencv-python

```

---

## ğŸ“‚ Dataset Setup

Ensure the following files are inside **datasets/**:

1. `holistic_49_keypoints.pkl`
2. `gloss_map.csv`

These files contain the pre-extracted 49-keypoint sequences for the Phoenix-2014T subset.

---

## â–¶ï¸ How to Run

### **1. Generate a Sign Animation**

Run the main pipeline to convert text to a sign language animation.

```bash
python scripts/run_pipeline.py

```

*Modify `text` variable in the script to change the input (e.g., "HELLO WORLD").*

### **2. Generate Ground Truth (Visual Twin)**

Render the original human motion with the exact same visual style as the AI output.

```bash
python render_gt.py hello_world

```

### **3. Run Quantitative Evaluation (Metrics)**

Calculate scientific accuracy scores (MPJPE, DTW, FID).

```bash
python evaluate.py hello_world

```

**Output Example:**

* **MPJPE (Position Error):** `0.5368` (Geometric Accuracy)
* **MPJAE (Angle Error):** `20.38Â°` (Articulation Correctness)
* **DTW (Time Warping):** `2.33` (Temporal Alignment)
* **FID (Visual Quality):** `122.66` (Generative Fidelity)

### **4. Visual Comparison (Side-by-Side)**

Generate a split-screen video to visually verify the result.

```bash
# Uses FFmpeg to stitch videos
ffmpeg -i output/hello_world.mp4 -i output/hello_world_gt.mp4 -filter_complex "[0:v][1:v]hstack=inputs=2[v]" -map "[v]" output/final_comparison.mp4

```

---

## ğŸ§  Pipeline Stages

### **1ï¸âƒ£ Load & Preprocess**

`loader.py` & `preprocess.py`

* Loads raw keypoints.
* Removes artifacts using velocity checks.
* Smooths jitter using B-Spline interpolation.

### **2ï¸âƒ£ Text Processing**

`translate.py`

* Maps English sentences to Gloss Sequences (e.g., "Good Morning" â†’ `GOOD_MORNING_GLOSS`).

### **3ï¸âƒ£ Sequence Construction**

`builder.py`

* Concatenates glosses into a continuous animation stream.
* Handles transitions between words.

### **4ï¸âƒ£ High-Fidelity Rendering**

`renderer.py`

* Renders the skeleton using a **Rainbow Topology**:
* **Thumb:** Red
* **Index:** Green
* **Middle:** Blue
* **Ring:** Pink
* **Pinky:** Yellow


* Uses "stacked lines" for aesthetic thickness and visibility.

---

## ğŸ¯ Purpose

This repository provides:

1. **Reproducibility:** A clear, step-by-step pipeline from text to video.
2. **Visual Clarity:** Distinct coloring helps researchers analyze finger articulation.
3. **Validation:** The `evaluate.py` module provides research-grade metrics to prove model accuracy.

---

## ğŸ™Œ Credits

* **MediaPipe Holistic** (Google)
* **Phoenix-2014-T Gloss Dataset** (RWTH Aachen University)
* **SLRT** (Fangyun Wei et al.) â€” structural inspiration

---
