# Spoken2Sign Pipeline (49-Landmark Sign Language Skeleton)

A clean, modular, SLRT-style Python pipeline for generating sign-language skeleton animations (49 MediaPipe Holistic landmarks) from spoken text.

**Now features a Scientific Verification Module (Ground Truth vs. Prediction).**

---

## ğŸŒ Overview

This project implements a functional **Spoken-to-Sign (S2S)** demonstration system with a built-in validation loop:

**English text â†’ Gloss sequence â†’ Preprocessed keypoints â†’ Skeleton animation (MP4)**

### Key Features:
- **49 MediaPipe Holistic landmarks** (21 left hand, 21 right hand, 7 upper body).
- **High-Fidelity Rendering:** Professional "stacked-line" visuals with **Rainbow Finger Encoding** for distinct articulation clarity.
- **Scientific Verification:** A side-by-side comparison module that validates AI output against human Ground Truth videos from the Phoenix-2014T dataset.
- **Physics-Based Processing:** Velocity filtering, interpolation, and smoothing for natural motion.

---

## ğŸ“ Repository Structure

```text
spoken2sign-pipeline/
â”‚
â”œâ”€â”€ compare.py             # ğŸ”¬ Scientific Validation (AI vs. Human Side-by-Side)
â”‚
â”œâ”€â”€ modules/
â”‚   â”œâ”€â”€ loader.py          # Load PKL keypoints & gloss CSV
â”‚   â”œâ”€â”€ preprocess.py      # Velocity filtering, interpolation, smoothing
â”‚   â”œâ”€â”€ translate.py       # English â†’ gloss sequence
â”‚   â”œâ”€â”€ builder.py         # Gloss â†’ keypoint sequence builder
â”‚   â”œâ”€â”€ renderer.py        # High-quality rainbow skeleton renderer
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ run_pipeline.py    # Main pipeline: text â†’ MP4 animation
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ default.yaml       # File paths & rendering settings
â”‚
â”œâ”€â”€ datasets/
â”‚   â”œâ”€â”€ holistic_49_keypoints.pkl  # Extracted pose data
â”‚   â””â”€â”€ gloss_map.csv              # Mapping: Gloss ID <-> Video ID
â”‚
â”œâ”€â”€ output/
â”‚   â”œâ”€â”€ hello_world.mp4    # Generated AI Video
â”‚   â””â”€â”€ compare_hello.mp4  # Proof Video (Side-by-Side)
â”‚
â””â”€â”€ README.md
```

---

## ğŸ”§ Installation

Clone the repository:

```bash
git clone https://github.com/<your-username>/spoken2sign-pipeline
cd spoken2sign-pipeline
```

Install required packages:

```bash
pip install numpy matplotlib scipy pyyaml
```

---

## ğŸ“‚ Dataset Setup

Ensure the following files are inside **datasets/**:

1. `holistic_49_keypoints.pkl`
2. `gloss_map.csv`

These files contain the pre-extracted 49-keypoint sequences for the Phoenix-2014T subset.

---

## â–¶ï¸ How to Run

### **1. Generate a Sign Animation**

Run the main pipeline to convert text to a sign language animation.

```bash
python scripts/run_pipeline.py
```

*Modify `text` variable in the script to change the input (e.g., "HELLO WORLD").*

### **2. Run Scientific Verification (Unit Test)**

To prove the model's accuracy, run the comparison script. This generates a side-by-side video of the **AI Prediction** (Left) vs. the **Human Ground Truth** (Right).

```bash
python compare.py hello
```

* **Left:** Predicted Sign Pose (Rainbow Hands, Orange Arms).
* **Right:** Ground Truth Source (from Dataset).
* **Result:** A synchronized `compare_hello.mp4` showing exact motion matching.

---

## ğŸ§  Pipeline Stages

### **1ï¸âƒ£ Load & Preprocess**

`loader.py` & `preprocess.py`

* Loads raw keypoints.
* Removes artifacts using velocity checks.
* Smooths jitter using B-Spline interpolation.

### **2ï¸âƒ£ Text Processing**

`translate.py`

* Maps English sentences to Gloss Sequences (e.g., "Good Morning" â†’ `GOOD_MORNING_GLOSS`).

### **3ï¸âƒ£ Sequence Construction**

`builder.py`

* Concatenates glosses into a continuous animation stream.
* Handles transitions between words.

### **4ï¸âƒ£ High-Fidelity Rendering**

`renderer.py`

* Renders the skeleton using a **Rainbow Topology**:
  * **Thumb:** Red
  * **Index:** Green
  * **Middle:** Blue
  * **Ring:** Pink
  * **Pinky:** Yellow

* Uses "stacked lines" for aesthetic thickness and visibility.

### **5ï¸âƒ£ Verification**

`compare.py`

* Locates the original Human video ID from the CSV.
* Synchronizes the AI output with the Human input.
* Renders a split-screen proof video with Sequence IDs.

---

## ğŸ¯ Purpose

This repository provides:

1. **Reproducibility:** A clear, step-by-step pipeline from text to video.
2. **Visual Clarity:** Distinct coloring helps researchers analyze finger articulation.
3. **Validation:** The `compare.py` module provides qualitative proof of the model's learning accuracy.

---

## ğŸ™Œ Credits

* **MediaPipe Holistic** (Google)
* **Phoenix-2014-T Gloss Dataset** (RWTH Aachen University)
* **SLRT** (Fangyun Wei et al.) â€” structural inspiration

---
